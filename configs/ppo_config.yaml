learning_rate: 5e-7  # <-- typo here
batch_size: 1
mini_batch_size: 1
gradient_accumulation_steps: 2
total_episodes: 100
logging_steps: 10
stop_token_id: 1  # To be updated dynamically after loading the tokenizer
