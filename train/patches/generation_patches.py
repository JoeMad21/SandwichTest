import torch
import types


def patch_generation_methods(model, tokenizer):
    """Apply all generation-related patches to the model."""
    
    # Patch the model's generate method
    original_generate = model.generate
    
    def ultra_safe_generate(self, *args, **kwargs):
        """Ultra-safe generate that forces greedy decoding"""
        print("ðŸ›¡ï¸  Using ultra-safe model generate override")
        
        # Force ultra-conservative parameters
        kwargs.update({
            'do_sample': False,
            'max_new_tokens': 5,
            'temperature': None,
            'top_k': None,
            'top_p': None,
            'num_beams': 1,
            'pad_token_id': tokenizer.pad_token_id,
            'eos_token_id': tokenizer.eos_token_id,
        })
        
        # Remove any sampling-related parameters
        for key in ['temperature', 'top_k', 'top_p', 'typical_p', 'epsilon_cutoff', 'eta_cutoff']:
            kwargs.pop(key, None)
        
        try:
            return original_generate(*args, **kwargs)
        except Exception as e:
            print(f"âš ï¸  Even ultra-safe generate failed: {e}")
            # Return input + EOS as absolute fallback
            input_ids = args[0] if args else kwargs.get('input_ids')
            if input_ids is not None:
                batch_size = input_ids.shape[0]
                eos_tokens = torch.full((batch_size, 1), tokenizer.eos_token_id,
                                      dtype=input_ids.dtype, device=input_ids.device)
                return torch.cat([input_ids, eos_tokens], dim=1)
            else:
                raise e
    
    # Bind the safe generate method to the model
    model.generate = types.MethodType(ultra_safe_generate, model)
    print("ðŸ›¡ï¸  Applied ultra-safe generate override to model")
    
    # Override generation config
    if hasattr(model, 'generation_config'):
        model.generation_config.max_new_tokens = 10
        model.generation_config.do_sample = False
        model.generation_config.pad_token_id = tokenizer.pad_token_id
        model.generation_config.eos_token_id = tokenizer.eos_token_id
        model.generation_config.use_cache = True
        model.generation_config.num_beams = 1


def create_ultra_safe_batch_generation(tokenizer):
    """Create an ultra-safe batch generation function."""
    
    def ultra_safe_batch_generation(*args, **kwargs):
        """Ultra-safe version that handles any parameter order"""
        print("ðŸ”§ Using ultra-safe batch generation - GREEDY ONLY")
        
        # Debug: Check what we actually received
        print(f"  Args: {len(args)} arguments")
        for i, arg in enumerate(args):
            print(f"    Arg {i}: {type(arg)}")
        print(f"  Kwargs: {kwargs}")
        
        try:
            # Find the actual parameters from the mixed-up arguments
            model = None
            queries = None
            generation_config = None
            pad_token_id = None
            
            # Try to identify parameters by type
            for arg in args:
                if hasattr(arg, 'generate') and hasattr(arg, 'forward'):  # Model
                    model = arg
                elif torch.is_tensor(arg):  # Tensor (likely queries)
                    queries = arg
                elif isinstance(arg, int):  # Integer (likely pad_token_id)
                    if pad_token_id is None:
                        pad_token_id = arg
                elif hasattr(arg, 'max_new_tokens'):  # GenerationConfig
                    generation_config = arg
            
            # Also check kwargs
            model = kwargs.get('model', model)
            queries = kwargs.get('queries', queries)
            generation_config = kwargs.get('generation_config', generation_config)
            pad_token_id = kwargs.get('pad_token_id', pad_token_id)
            
            print(f"  Identified - Model: {type(model)}, Queries: {type(queries)}")
            
            # If we can't find essential components, fall back immediately
            if model is None or queries is None:
                print("âš ï¸  Cannot identify model or queries, using fallback")
                raise ValueError("Cannot identify essential parameters")
            
            # Get tokenizer IDs
            actual_eos_token_id = tokenizer.eos_token_id if tokenizer else 50256
            
            # Try to use the original function first
            import trl.trainer.utils as trl_utils
            if hasattr(trl_utils, '_original_batch_generation'):
                try:
                    result = trl_utils._original_batch_generation(*args, **kwargs)
                    print("âœ… Original function succeeded")
                    return result
                except Exception as e:
                    print(f"âš ï¸  Original function failed: {e}")
            
            # Fallback: create minimal response
            raise ValueError("Forcing fallback response")
            
        except Exception as e:
            print(f"âš ï¸  All generation attempts failed: {e}")
            print(f"âš ï¸  Creating minimal dummy response")
            
            # Find the queries tensor
            queries_tensor = None
            for arg in args:
                if torch.is_tensor(arg):
                    queries_tensor = arg
                    break
            
            if queries_tensor is None:
                queries_tensor = torch.tensor([[50256]], dtype=torch.long)
                if torch.cuda.is_available():
                    queries_tensor = queries_tensor.cuda()
            
            # Create minimal response
            batch_size = queries_tensor.shape[0] if len(queries_tensor.shape) > 1 else 1
            seq_len = queries_tensor.shape[1] if len(queries_tensor.shape) > 1 else 1
            
            if len(queries_tensor.shape) == 1:
                queries_tensor = queries_tensor.unsqueeze(0)
            
            eos_tokens = torch.full((batch_size, 1), tokenizer.eos_token_id,
                                  dtype=queries_tensor.dtype, device=queries_tensor.device)
            dummy_response = torch.cat([queries_tensor, eos_tokens], dim=1)
            
            # Create minimal dummy logits
            vocab_size = 51200  # phi-2 vocab size
            dummy_logits = torch.zeros((batch_size, seq_len + 1, vocab_size),
                                     dtype=torch.float, device=queries_tensor.device)
            
            return dummy_response, dummy_logits
    
    return ultra_safe_batch_generation
